---
title: "2024-08-23-Datawhale_AIå¤ä»¤è¥_åŠ¨æ‰‹å­¦å¤§æ¨¡å‹åº”ç”¨å…¨æ ˆå¼€å‘_Task1"
author: "BlackJack0083"
date: "2024-08-23"
toc: true
tags: ["å¤§æ¨¡å‹"]
comments: true
---

# åŸºæœ¬æ­¥éª¤

### step0
- å¼€é€šé˜¿é‡Œäº‘ PAI-DSWè¯•ç”¨[https://free.aliyun.com/?productCode=learn](https://free.aliyun.com/?productCode=learn)
- åœ¨é­”æ­ç¤¾åŒºæˆæƒ https://www.modelscope.cn/my/mynotebook/authorization

### step1
åœ¨é­”æ­ç¤¾åŒºåˆ›å»ºPAIå®ä¾‹ https://www.modelscope.cn/my/mynotebook/authorization

### step2 Demo æ­å»º

#### æ–‡ä»¶ä¸‹è½½

åœ¨ç»ˆç«¯è¾“å…¥å¦‚ä¸‹æŒ‡ä»¤ï¼š

```Bash
git lfs install
git clone https://www.modelscope.cn/datasets/Datawhale/AICamp_yuan_baseline.git
```

#### ç¯å¢ƒå®‰è£…

```Bash
pip install streamlit==1.24.0
```

#### å¯åŠ¨Demo

```Bash
streamlit run AICamp_yuan_baseline/Task\ 1ï¼šé›¶åŸºç¡€ç©è½¬æºå¤§æ¨¡å‹/web_demo_2b.py --server.address 127.0.0.1 --server.port 6006
```

### å¯¹è¯ä½“éªŒ

æ¨¡å‹åŠ è½½å®Œåå³å¯è¿›è¡Œå¯¹è¯

# baseline ç²¾è¯»

```Python
# å¯¼å…¥æ‰€éœ€çš„åº“
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import streamlit as st

# åˆ›å»ºä¸€ä¸ªæ ‡é¢˜å’Œä¸€ä¸ªå‰¯æ ‡é¢˜
st.title("ğŸ’¬ Yuan2.0 æ™ºèƒ½ç¼–ç¨‹åŠ©æ‰‹")

# æºå¤§æ¨¡å‹ä¸‹è½½
from modelscope import snapshot_download
model_dir = snapshot_download('IEITYuan/Yuan2-2B-Mars-hf', cache_dir='./')
# model_dir = snapshot_download('IEITYuan/Yuan2-2B-July-hf', cache_dir='./')
```
**æ¨¡å‹ä¸‹è½½ï¼š**
Yuan2-2B-Marsæ”¯æŒé€šè¿‡å¤šä¸ªå¹³å°è¿›è¡Œä¸‹è½½ï¼ŒåŒ…æ‹¬é­”æ­ã€HuggingFaceã€OpenXlabã€ç™¾åº¦ç½‘ç›˜ã€WiseModelç­‰ã€‚å› ä¸ºæˆ‘ä»¬çš„æœºå™¨å°±åœ¨é­”æ­ï¼Œæ‰€ä»¥è¿™é‡Œæˆ‘ä»¬ç›´æ¥é€‰æ‹©é€šè¿‡é­”æ­è¿›è¡Œä¸‹è½½ã€‚æ¨¡å‹åœ¨é­”æ­å¹³å°çš„åœ°å€ä¸º [IEITYuan/Yuan2-2B-Mars-hf](https://modelscope.cn/models/IEITYuan/Yuan2-2B-Mars-hf)ã€‚

æ¨¡å‹ä¸‹è½½ä½¿ç”¨çš„æ˜¯ modelscope ä¸­çš„ snapshot_download å‡½æ•°ï¼Œç¬¬ä¸€ä¸ªå‚æ•°ä¸ºæ¨¡å‹åç§° `IEITYuan/Yuan2-2B-Mars-hf`ï¼Œç¬¬äºŒä¸ªå‚æ•° `cache_dir` ä¸ºæ¨¡å‹ä¿å­˜è·¯å¾„ï¼Œè¿™é‡Œ`.`è¡¨ç¤ºå½“å‰è·¯å¾„ã€‚

æ¨¡å‹å¤§å°çº¦ä¸º4.1Gï¼Œç”±äºæ˜¯ä»é­”æ­ç›´æ¥è¿›è¡Œä¸‹è½½ï¼Œé€Ÿåº¦ä¼šéå¸¸å¿«ã€‚ä¸‹è½½å®Œæˆåï¼Œä¼šåœ¨å½“å‰ç›®å½•å¢åŠ ä¸€ä¸ªåä¸º `IEITYuan` çš„æ–‡ä»¶å¤¹ï¼Œå…¶ä¸­ `Yuan2-2B-Mars-hf` é‡Œé¢ä¿å­˜ç€æˆ‘ä»¬ä¸‹è½½å¥½çš„æºå¤§æ¨¡å‹ã€‚

```python
# å®šä¹‰æ¨¡å‹è·¯å¾„
path = './IEITYuan/Yuan2-2B-Mars-hf'
# path = './IEITYuan/Yuan2-2B-July-hf'

# å®šä¹‰æ¨¡å‹æ•°æ®ç±»å‹
torch_dtype = torch.bfloat16 # A10
# torch_dtype = torch.float16 # P100
```

åŸºæœ¬ç±»å‹ã€è·¯å¾„ç¡®å®šä¸åŠ è½½

```python
# å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œç”¨äºè·å–æ¨¡å‹å’Œtokenizer
@st.cache_resource
# è£…é¥°å™¨ï¼Œç¼“å­˜åŠ è½½å¥½çš„æ¨¡å‹å’Œtokenizer
def get_model():
    print("Creat tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(path, add_eos_token=False, add_bos_token=False, eos_token='<eod>')
    tokenizer.add_tokens(['<sep>', '<pad>', '<mask>', '<predict>', '<FIM_SUFFIX>', '<FIM_PREFIX>', '<FIM_MIDDLE>','<commit_before>','<commit_msg>','<commit_after>','<jupyter_start>','<jupyter_text>','<jupyter_code>','<jupyter_output>','<empty_output>'], special_tokens=True)

    print("Creat model...")
    model = AutoModelForCausalLM.from_pretrained(path, torch_dtype=torch_dtype, trust_remote_code=True).cuda()

    print("Done.")
    return tokenizer, model
```

ä½¿ç”¨ `transformers` ä¸­çš„ `from_pretrained` å‡½æ•°æ¥åŠ è½½ä¸‹è½½å¥½çš„æ¨¡å‹å’Œtokenizerï¼Œå¹¶é€šè¿‡ `.cuda()` å°†æ¨¡å‹æ”¾ç½®åœ¨GPUä¸Šã€‚å¦å¤–ï¼Œè¿™é‡Œé¢å¤–ä½¿ç”¨äº† `streamlit` æä¾›çš„ä¸€ä¸ªè£…é¥°å™¨ `@st.cache_resource` ï¼Œå®ƒå¯ä»¥ç”¨äºç¼“å­˜åŠ è½½å¥½çš„æ¨¡å‹å’Œtokenizerã€‚

```python
# åŠ è½½modelå’Œtokenizer
tokenizer, model = get_model()

# åˆæ¬¡è¿è¡Œæ—¶ï¼Œsession_stateä¸­æ²¡æœ‰"messages"ï¼Œéœ€è¦åˆ›å»ºä¸€ä¸ªç©ºåˆ—è¡¨
if "messages" not in st.session_state:
    st.session_state["messages"] = []

# æ¯æ¬¡å¯¹è¯æ—¶ï¼Œéƒ½éœ€è¦éå†session_stateä¸­çš„æ‰€æœ‰æ¶ˆæ¯ï¼Œå¹¶æ˜¾ç¤ºåœ¨èŠå¤©ç•Œé¢ä¸Š
for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])
```

åˆå§‹åŒ–å’Œä¸æ–­æ›´æ–°èŠå¤©ç•Œé¢

```python
# å¦‚æœç”¨æˆ·åœ¨èŠå¤©è¾“å…¥æ¡†ä¸­è¾“å…¥äº†å†…å®¹ï¼Œåˆ™æ‰§è¡Œä»¥ä¸‹æ“ä½œ
if prompt := st.chat_input():
    # å°†ç”¨æˆ·çš„è¾“å…¥æ·»åŠ åˆ°session_stateä¸­çš„messagesåˆ—è¡¨ä¸­
    st.session_state.messages.append({"role": "user", "content": prompt})

    # åœ¨èŠå¤©ç•Œé¢ä¸Šæ˜¾ç¤ºç”¨æˆ·çš„è¾“å…¥
    st.chat_message("user").write(prompt)

    # è°ƒç”¨æ¨¡å‹
    prompt = "<n>".join(msg["content"] for msg in st.session_state.messages) + "<sep>" # æ‹¼æ¥å¯¹è¯å†å²
    inputs = tokenizer(prompt, return_tensors="pt")["input_ids"].cuda()
    outputs = model.generate(inputs, do_sample=False, max_length=1024) # è®¾ç½®è§£ç æ–¹å¼å’Œæœ€å¤§ç”Ÿæˆé•¿åº¦
    output = tokenizer.decode(outputs[0])
    response = output.split("<sep>")[-1].replace("<eod>", '')

    # å°†æ¨¡å‹çš„è¾“å‡ºæ·»åŠ åˆ°session_stateä¸­çš„messagesåˆ—è¡¨ä¸­
    st.session_state.messages.append({"role": "assistant", "content": response})

    # åœ¨èŠå¤©ç•Œé¢ä¸Šæ˜¾ç¤ºæ¨¡å‹çš„è¾“å‡º
    st.chat_message("assistant").write(response)
```

è¿™æ®µä»£ç çš„ç›®çš„æ˜¯åœ¨ä¸€ä¸ªèŠå¤©åº”ç”¨ä¸­ï¼Œå¤„ç†ç”¨æˆ·çš„è¾“å…¥ï¼Œè°ƒç”¨æ¨¡å‹ç”Ÿæˆå›å¤ï¼Œå¹¶å°†åŒæ–¹çš„å¯¹è¯æ˜¾ç¤ºåœ¨ç•Œé¢ä¸Šã€‚ä¸‹é¢æ˜¯ä»£ç é€æ­¥è§£é‡Šï¼š

1. **ç”¨æˆ·è¾“å…¥æ£€æµ‹**ï¼š
    ```python
    if prompt := st.chat_input():
    ```
    è¿™ä¸€è¡Œä½¿ç”¨äº†æµ·è±¡è¿ç®—ç¬¦ `:=`ï¼Œè¡¨ç¤ºå¦‚æœç”¨æˆ·åœ¨èŠå¤©è¾“å…¥æ¡†ä¸­è¾“å…¥äº†å†…å®¹ï¼ˆå³ `st.chat_input()` è¿”å›äº†éç©ºå­—ç¬¦ä¸²ï¼‰ï¼Œåˆ™å°†è¾“å…¥èµ‹å€¼ç»™ `prompt` å˜é‡ï¼Œå¹¶è¿›å…¥ä»£ç å—ç»§ç»­æ‰§è¡Œã€‚

2. **å­˜å‚¨ç”¨æˆ·è¾“å…¥**ï¼š
    ```python
    st.session_state.messages.append({"role": "user", "content": prompt})
    ```
    è¿™è¡Œä»£ç å°†ç”¨æˆ·çš„è¾“å…¥ä¿å­˜åˆ° `st.session_state.messages` åˆ—è¡¨ä¸­ï¼Œ`messages` åˆ—è¡¨ä¸­çš„æ¯ä¸€æ¡æ¶ˆæ¯æ˜¯ä¸€ä¸ªå­—å…¸ï¼ŒåŒ…å« `role`ï¼ˆç”¨æˆ·èº«ä»½ï¼‰å’Œ `content`ï¼ˆæ¶ˆæ¯å†…å®¹ï¼‰çš„é”®å€¼å¯¹ã€‚åœ¨è¿™é‡Œï¼Œ`role` è¢«è®¾ç½®ä¸º `"user"`ï¼Œå†…å®¹æ˜¯ç”¨æˆ·è¾“å…¥çš„ `prompt`ã€‚

3. **åœ¨ç•Œé¢æ˜¾ç¤ºç”¨æˆ·è¾“å…¥**ï¼š
    ```python
    st.chat_message("user").write(prompt)
    ```
    ä½¿ç”¨ `st.chat_message("user").write(prompt)` åœ¨èŠå¤©ç•Œé¢ä¸Šæ˜¾ç¤ºç”¨æˆ·è¾“å…¥çš„å†…å®¹ã€‚

4. **æ‹¼æ¥å¯¹è¯å†å²**ï¼š
    ```python
    prompt = "<n>".join(msg["content"] for msg in st.session_state.messages) + "<sep>"
    ```
    è¿™ä¸€è¡Œä»£ç é€šè¿‡éå† `st.session_state.messages` åˆ—è¡¨ï¼Œå°†æ‰€æœ‰æ¶ˆæ¯çš„ `content`ï¼ˆæ¶ˆæ¯å†…å®¹ï¼‰æ‹¼æ¥èµ·æ¥ï¼Œä½¿ç”¨ `<n>` ä½œä¸ºæ¶ˆæ¯ä¹‹é—´çš„åˆ†éš”ç¬¦ï¼Œå¹¶åœ¨æœ€åé™„åŠ ä¸Š `<sep>`ï¼Œè¡¨ç¤ºç»“æŸç¬¦å·ã€‚è¿™ä¸€æ­¥æ˜¯ä¸ºäº†æ„å»ºå®Œæ•´çš„å¯¹è¯ä¸Šä¸‹æ–‡ï¼Œä¾›æ¨¡å‹ä½¿ç”¨ã€‚

5. **å‡†å¤‡è¾“å…¥æ•°æ®**ï¼š
    ```python
    inputs = tokenizer(prompt, return_tensors="pt")["input_ids"].cuda()
    ```
    å°†ä¸Šä¸€æ­¥æ‹¼æ¥çš„ `prompt` é€šè¿‡ `tokenizer` è¿›è¡Œåˆ†è¯å¤„ç†ï¼ˆ`tokenizer` æ˜¯ä¸€ä¸ªåˆ†è¯å™¨å¯¹è±¡ï¼‰ï¼Œå¹¶è¿”å›å¼ é‡å½¢å¼çš„è¾“å…¥æ•°æ®ï¼ˆ`return_tensors="pt"` è¡¨ç¤ºè¿”å› PyTorch å¼ é‡ï¼‰ï¼Œç„¶åå°†å…¶ç§»åŠ¨åˆ° GPU ä¸Šï¼ˆé€šè¿‡ `.cuda()`ï¼‰ã€‚

6. **æ¨¡å‹ç”Ÿæˆå›å¤**ï¼š
    ```python
    outputs = model.generate(inputs, do_sample=False, max_length=1024)
    ```
    è°ƒç”¨æ¨¡å‹çš„ `generate` æ–¹æ³•ç”Ÿæˆå›å¤ã€‚`do_sample=False` è¡¨ç¤ºä½¿ç”¨è´ªå¿ƒæœç´¢ï¼ˆä¸è¿›è¡Œé‡‡æ ·ï¼‰ï¼Œ`max_length=1024` è®¾ç½®ç”Ÿæˆçš„æœ€å¤§é•¿åº¦ä¸º 1024 ä¸ª tokenã€‚

7. **å¤„ç†æ¨¡å‹è¾“å‡º**ï¼š
    ```python
    output = tokenizer.decode(outputs[0])
    response = output.split("<sep>")[-1].replace("<eod>", '')
    ```
    æ¨¡å‹çš„è¾“å‡ºæ˜¯ä¸€ä¸ª token åºåˆ—ï¼Œé¦–å…ˆä½¿ç”¨ `tokenizer.decode` å°†å…¶è§£ç ä¸ºå¯è¯»çš„æ–‡æœ¬ã€‚æ¥ä¸‹æ¥ï¼Œ`output.split("<sep>")[-1]` ä»ç”Ÿæˆçš„æ–‡æœ¬ä¸­æå–å‡º `<sep>` åçš„å†…å®¹ï¼Œè¡¨ç¤ºæœ€ç»ˆå›å¤ï¼Œ`replace("<eod>", '')` åˆ™å»é™¤ç»“æŸæ ‡å¿— `<eod>`ã€‚

8. **å­˜å‚¨å’Œæ˜¾ç¤ºæ¨¡å‹çš„å›å¤**ï¼š
    ```python
    st.session_state.messages.append({"role": "assistant", "content": response})
    st.chat_message("assistant").write(response)
    ```
    å°†æ¨¡å‹ç”Ÿæˆçš„å›å¤ä»¥ç±»ä¼¼äºç”¨æˆ·è¾“å…¥çš„æ–¹å¼ä¿å­˜åˆ° `st.session_state.messages` åˆ—è¡¨ä¸­ï¼Œ`role` è¢«è®¾ç½®ä¸º `"assistant"`ï¼Œ`content` æ˜¯æ¨¡å‹ç”Ÿæˆçš„ `response`ã€‚éšåä½¿ç”¨ `st.chat_message("assistant").write(response)` åœ¨ç•Œé¢ä¸Šæ˜¾ç¤ºåŠ©æ‰‹çš„å›å¤ã€‚

---

æ€»ç»“ï¼š
- è¿™æ®µä»£ç çš„æ ¸å¿ƒæµç¨‹æ˜¯ï¼šå½“ç”¨æˆ·è¾“å…¥å†…å®¹æ—¶ï¼Œç³»ç»Ÿä¼šä¿å­˜ç”¨æˆ·çš„è¾“å…¥å¹¶åœ¨ç•Œé¢ä¸Šæ˜¾ç¤ºï¼Œç„¶åä½¿ç”¨æ¨¡å‹ç”Ÿæˆå›å¤ï¼Œæœ€ç»ˆä¿å­˜å¹¶æ˜¾ç¤ºæ¨¡å‹çš„è¾“å‡ºã€‚
- è¯¥ä»£ç ä¾èµ–äºä¸€ä¸ªåˆ†è¯å™¨ï¼ˆ`tokenizer`ï¼‰å’Œæ¨¡å‹ï¼ˆ`model`ï¼‰ï¼Œå¹¶ä½¿ç”¨ `streamlit`ï¼ˆç®€ç§°ä¸º `st`ï¼‰è¿›è¡Œç•Œé¢çš„äº¤äº’å’Œæ˜¾ç¤ºã€‚

# RAG

## 1 å¼•è¨€

### 1.1 ä»€ä¹ˆæ˜¯RAG

åœ¨ä¸Šä¸€èŠ‚ï¼Œæˆ‘ä»¬æˆåŠŸæ­å»ºäº†ä¸€ä¸ªæºå¤§æ¨¡å‹æ™ºèƒ½å¯¹è¯Demoï¼Œäº²èº«ä½“éªŒåˆ°äº†å¤§æ¨¡å‹å‡ºè‰²çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œåœ¨å®é™…ä¸šåŠ¡åœºæ™¯ä¸­ï¼Œé€šç”¨çš„åŸºç¡€å¤§æ¨¡å‹å¯èƒ½å­˜åœ¨æ— æ³•æ»¡è¶³æˆ‘ä»¬éœ€æ±‚çš„æƒ…å†µï¼Œä¸»è¦æœ‰ä»¥ä¸‹å‡ æ–¹é¢åŸå› ï¼š

- çŸ¥è¯†å±€é™æ€§ï¼šå¤§æ¨¡å‹çš„çŸ¥è¯†æ¥æºäºè®­ç»ƒæ•°æ®ï¼Œè€Œè¿™äº›æ•°æ®ä¸»è¦æ¥è‡ªäºäº’è”ç½‘ä¸Šå·²ç»å…¬å¼€çš„èµ„æºï¼Œå¯¹äºä¸€äº›å®æ—¶æ€§çš„æˆ–è€…éå…¬å¼€çš„ï¼Œç”±äºå¤§æ¨¡å‹æ²¡æœ‰è·å–åˆ°ç›¸å…³æ•°æ®ï¼Œè¿™éƒ¨åˆ†çŸ¥è¯†ä¹Ÿå°±æ— æ³•è¢«æŒæ¡ã€‚
- æ•°æ®å®‰å…¨æ€§ï¼šä¸ºäº†ä½¿å¾—å¤§æ¨¡å‹èƒ½å¤Ÿå…·å¤‡ç›¸åº”çš„çŸ¥è¯†ï¼Œå°±éœ€è¦å°†æ•°æ®çº³å…¥åˆ°è®­ç»ƒé›†è¿›è¡Œè®­ç»ƒã€‚ç„¶è€Œï¼Œå¯¹äºä¼ä¸šæ¥è¯´ï¼Œæ•°æ®çš„å®‰å…¨æ€§è‡³å…³é‡è¦ï¼Œä»»ä½•å½¢å¼çš„æ•°æ®æ³„éœ²éƒ½å¯èƒ½å¯¹ä¼ä¸šæ„æˆè‡´å‘½çš„å¨èƒã€‚
- å¤§æ¨¡å‹å¹»è§‰ï¼šç”±äºå¤§æ¨¡å‹æ˜¯åŸºäºæ¦‚ç‡ç»Ÿè®¡è¿›è¡Œæ„å»ºçš„ï¼Œå…¶è¾“å‡ºæœ¬è´¨ä¸Šæ˜¯ä¸€ç³»åˆ—æ•°å€¼è¿ç®—ã€‚å› æ­¤ï¼Œæœ‰æ—¶ä¼šå‡ºç°æ¨¡å‹â€œä¸€æœ¬æ­£ç»åœ°èƒ¡è¯´å…«é“â€çš„æƒ…å†µï¼Œå°¤å…¶æ˜¯åœ¨å¤§æ¨¡å‹ä¸å…·å¤‡çš„çŸ¥è¯†æˆ–ä¸æ“…é•¿çš„åœºæ™¯ä¸­ã€‚

ä¸ºäº†ä¸Šè¿°è¿™äº›é—®é¢˜ï¼Œç ”ç©¶äººå‘˜æå‡ºäº†æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRetrieval Augmented Generation, **RAG**ï¼‰çš„æ–¹æ³•ã€‚è¿™ç§æ–¹æ³•é€šè¿‡å¼•å…¥å¤–éƒ¨çŸ¥è¯†ï¼Œä½¿å¤§æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆå‡†ç¡®ä¸”ç¬¦åˆä¸Šä¸‹æ–‡çš„ç­”æ¡ˆï¼ŒåŒæ—¶èƒ½å¤Ÿå‡å°‘æ¨¡å‹å¹»è§‰çš„å‡ºç°ã€‚

ç”±äºRAGç®€å•æœ‰æ•ˆï¼Œå®ƒå·²ç»æˆä¸ºä¸»æµçš„å¤§æ¨¡å‹åº”ç”¨æ–¹æ¡ˆä¹‹ä¸€ã€‚
å¦‚ä¸‹å›¾æ‰€ç¤ºï¼ŒRAGé€šå¸¸åŒ…æ‹¬ä»¥ä¸‹ä¸‰ä¸ªåŸºæœ¬æ­¥éª¤ï¼š

![image.png](https://cdn.jsdelivr.net/gh/BlackJack0083/image@main/img/20240825130004.png)

- ç´¢å¼•ï¼šå°†æ–‡æ¡£åº“åˆ†å‰²æˆè¾ƒçŸ­çš„ **Chunk**ï¼Œå³æ–‡æœ¬å—æˆ–æ–‡æ¡£ç‰‡æ®µï¼Œç„¶åæ„å»ºæˆå‘é‡ç´¢å¼•ã€‚
- æ£€ç´¢ï¼šè®¡ç®—é—®é¢˜å’Œ Chunks çš„ç›¸ä¼¼åº¦ï¼Œæ£€ç´¢å‡ºè‹¥å¹²ä¸ªç›¸å…³çš„ Chunkã€‚
- ç”Ÿæˆï¼šå°†æ£€ç´¢åˆ°çš„Chunksä½œä¸ºèƒŒæ™¯ä¿¡æ¯ï¼Œç”Ÿæˆé—®é¢˜çš„å›ç­”ã€‚

### 1.2 ä¸€ä¸ªå®Œæ•´çš„RAGé“¾è·¯

æœ¬å°èŠ‚æˆ‘ä»¬å°†å¸¦å¤§å®¶æ„å»ºä¸€ä¸ªå®Œæ•´çš„RAGé“¾è·¯ã€‚

![image.png|500](https://cdn.jsdelivr.net/gh/BlackJack0083/image@main/img/20240825130228.png)
ä»ä¸Šå›¾å¯ä»¥çœ‹åˆ°ï¼Œçº¿ä¸Šæ¥æ”¶åˆ°ç”¨æˆ·`query`åï¼ŒRAGä¼šå…ˆè¿›è¡Œæ£€ç´¢ï¼Œç„¶åå°†æ£€ç´¢åˆ°çš„ **`Chunks`** å’Œ **`query`** ä¸€å¹¶è¾“å…¥åˆ°å¤§æ¨¡å‹ï¼Œè¿›è€Œå›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

ä¸ºäº†å®Œæˆæ£€ç´¢ï¼Œéœ€è¦ç¦»çº¿å°†æ–‡æ¡£ï¼ˆpptã€wordã€pdfç­‰ï¼‰ç»è¿‡è§£æã€åˆ‡å‰²ç”šè‡³OCRè½¬å†™ï¼Œç„¶åè¿›è¡Œå‘é‡åŒ–å­˜å…¥æ•°æ®åº“ä¸­ã€‚

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†åˆ†åˆ«ä»‹ç»ç¦»çº¿è®¡ç®—å’Œåœ¨çº¿è®¡ç®—æµç¨‹ã€‚

#### 1.2.1 ç¦»çº¿è®¡ç®—

é¦–å…ˆï¼ŒçŸ¥è¯†åº“ä¸­åŒ…å«äº†å¤šç§ç±»å‹çš„æ–‡ä»¶ï¼Œå¦‚pdfã€wordã€pptç­‰ï¼Œè¿™äº› `æ–‡æ¡£`ï¼ˆDocumentsï¼‰éœ€è¦æå‰è¢«è§£æï¼Œç„¶ååˆ‡å‰²æˆè‹¥å¹²ä¸ªè¾ƒçŸ­çš„ `Chunk`ï¼Œå¹¶ä¸”è¿›è¡Œæ¸…æ´—å’Œå»é‡ã€‚

ç”±äºçŸ¥è¯†åº“ä¸­çŸ¥è¯†çš„æ•°é‡å’Œè´¨é‡å†³å®šäº†RAGçš„æ•ˆæœï¼Œå› æ­¤è¿™æ˜¯éå¸¸å…³é”®ä¸”å¿…ä¸å¯å°‘çš„ç¯èŠ‚ã€‚

ç„¶åï¼Œæˆ‘ä»¬ä¼šå°†çŸ¥è¯†åº“ä¸­çš„æ‰€æœ‰ `Chunk` éƒ½è½¬æˆå‘é‡ï¼Œè¿™ä¸€æ­¥ä¹Ÿç§°ä¸º `å‘é‡åŒ–`ï¼ˆVectorizationï¼‰æˆ–è€… `ç´¢å¼•`ï¼ˆIndexingï¼‰ã€‚

`å‘é‡åŒ–` éœ€è¦äº‹å…ˆæ„å»ºä¸€ä¸ª `å‘é‡æ¨¡å‹`ï¼ˆEmbedding Modelï¼‰ï¼Œå®ƒçš„ä½œç”¨å°±æ˜¯å°†ä¸€æ®µ `Chunk` è½¬æˆ `å‘é‡`ï¼ˆEmbeddingï¼‰ã€‚å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

![image.png](https://cdn.jsdelivr.net/gh/BlackJack0083/image@main/img/20240825130327.png)

ä¸€ä¸ªå¥½çš„å‘é‡æ¨¡å‹ï¼Œä¼šä½¿å¾—å…·æœ‰ç›¸åŒè¯­ä¹‰çš„æ–‡æœ¬çš„å‘é‡è¡¨ç¤ºåœ¨è¯­ä¹‰ç©ºé—´ä¸­çš„è·ç¦»ä¼šæ¯”è¾ƒè¿‘ï¼Œè€Œè¯­ä¹‰ä¸åŒçš„æ–‡æœ¬åœ¨è¯­ä¹‰ç©ºé—´ä¸­çš„è·ç¦»ä¼šæ¯”è¾ƒè¿œã€‚

![image.png](https://cdn.jsdelivr.net/gh/BlackJack0083/image@main/img/20240825130350.png)
ç”±äºçŸ¥è¯†åº“ä¸­çš„æ‰€æœ‰ `Chunk` éƒ½éœ€è¦è¿›è¡Œ `å‘é‡åŒ–`ï¼Œè¿™ä¼šä½¿å¾—è®¡ç®—é‡éå¸¸å¤§ï¼Œå› æ­¤è¿™ä¸€è¿‡ç¨‹é€šå¸¸æ˜¯ç¦»çº¿å®Œæˆçš„ã€‚

éšç€æ–°çŸ¥è¯†çš„ä¸æ–­å­˜å‚¨ï¼Œå‘é‡çš„æ•°é‡ä¹Ÿä¼šä¸æ–­å¢åŠ ã€‚è¿™å°±éœ€è¦å°†è¿™äº›å‘é‡å­˜å‚¨åˆ° `æ•°æ®åº“` ï¼ˆDataBaseï¼‰ä¸­è¿›è¡Œç®¡ç†ï¼Œä¾‹å¦‚ [Milvus](https://milvus.io/) ä¸­ã€‚

è‡³æ­¤ï¼Œç¦»çº¿è®¡ç®—å°±å®Œæˆäº†ã€‚

#### 1.2.2 åœ¨çº¿è®¡ç®—

åœ¨å®é™…ä½¿ç”¨RAGç³»ç»Ÿæ—¶ï¼Œå½“ç»™å®šä¸€æ¡ç”¨æˆ· `æŸ¥è¯¢`ï¼ˆQueryï¼‰ï¼Œéœ€è¦å…ˆä»çŸ¥è¯†åº“ä¸­æ‰¾åˆ°æ‰€éœ€çš„çŸ¥è¯†ï¼Œè¿™ä¸€æ­¥ç§°ä¸º `æ£€ç´¢`ï¼ˆRetrievalï¼‰ã€‚

åœ¨ `æ£€ç´¢` è¿‡ç¨‹ä¸­ï¼Œç”¨æˆ·æŸ¥è¯¢é¦–å…ˆä¼šç»è¿‡å‘é‡æ¨¡å‹å¾—åˆ°ç›¸åº”çš„å‘é‡ï¼Œç„¶åä¸ `æ•°æ®åº“` ä¸­æ‰€æœ‰ `Chunk` çš„å‘é‡è®¡ç®—ç›¸ä¼¼åº¦ï¼Œæœ€ç®€å•çš„ä¾‹å¦‚ `ä½™å¼¦ç›¸ä¼¼åº¦`ï¼Œç„¶åå¾—åˆ°æœ€ç›¸è¿‘çš„ä¸€ç³»åˆ— `Chunk` ã€‚

ç”±äºå‘é‡ç›¸ä¼¼åº¦çš„è®¡ç®—è¿‡ç¨‹éœ€è¦ä¸€å®šçš„æ—¶é—´ï¼Œå°¤å…¶æ˜¯ `æ•°æ®åº“` éå¸¸å¤§çš„æ—¶å€™ã€‚

è¿™æ—¶ï¼Œå¯ä»¥åœ¨æ£€ç´¢ä¹‹å‰è¿›è¡Œ `å¬å›`ï¼ˆRecallï¼‰ï¼Œå³ä» `æ•°æ®åº“` ä¸­å¿«é€Ÿè·å¾—å¤§é‡å¤§æ¦‚ç‡ç›¸å…³çš„ `Chunk`ï¼Œç„¶ååªæœ‰è¿™äº› `Chunk` ä¼šå‚ä¸è®¡ç®—å‘é‡ç›¸ä¼¼åº¦ã€‚è¿™æ ·ï¼Œè®¡ç®—çš„å¤æ‚åº¦å°±ä»æ•´ä¸ªçŸ¥è¯†åº“é™åˆ°äº†éå¸¸ä½ã€‚

`å¬å›` æ­¥éª¤ä¸è¦æ±‚éå¸¸é«˜çš„å‡†ç¡®æ€§ï¼Œå› æ­¤é€šå¸¸é‡‡ç”¨ç®€å•çš„åŸºäºå­—ç¬¦ä¸²çš„åŒ¹é…ç®—æ³•ã€‚ç”±äºè¿™äº›ç®—æ³•ä¸éœ€è¦ä»»ä½•æ¨¡å‹ï¼Œé€Ÿåº¦ä¼šéå¸¸å¿«ï¼Œå¸¸ç”¨çš„ç®—æ³•æœ‰ `TF-IDF`ï¼Œ`BM25` ç­‰ã€‚

å¦å¤–ï¼Œä¹Ÿæœ‰å¾ˆå¤šå·¥ä½œè‡´åŠ›äºå®ç°æ›´å¿«çš„ `å‘é‡æ£€ç´¢` ï¼Œä¾‹å¦‚ [faiss](https://github.com/facebookresearch/faiss)ï¼Œ[annoy](https://github.com/spotify/annoy)ã€‚ 

å¦ä¸€æ–¹é¢ï¼Œäººä»¬å‘ç°ï¼Œéšç€çŸ¥è¯†åº“çš„å¢å¤§ï¼Œé™¤äº†æ£€ç´¢çš„é€Ÿåº¦å˜æ…¢å¤–ï¼Œæ£€ç´¢çš„æ•ˆæœä¹Ÿä¼šå‡ºç°é€€åŒ–ï¼Œå¦‚ä¸‹å›¾ä¸­ç»¿çº¿æ‰€ç¤ºï¼š
![image.png](https://cdn.jsdelivr.net/gh/BlackJack0083/image@main/img/20240825130653.png)

è¿™æ˜¯ç”±äº `å‘é‡æ¨¡å‹` èƒ½åŠ›æœ‰é™ï¼Œè€Œéšç€çŸ¥è¯†åº“çš„å¢å¤§ï¼Œå·²ç»è¶…å‡ºäº†å…¶å®¹é‡ï¼Œå› æ­¤å‡†ç¡®æ€§å°±ä¼šä¸‹é™ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œç›¸ä¼¼åº¦æœ€é«˜çš„ç»“æœå¯èƒ½å¹¶ä¸æ˜¯æœ€ä¼˜çš„ã€‚

ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡RAGæ•ˆæœï¼Œç ”ç©¶è€…æå‡ºå¢åŠ ä¸€ä¸ªäºŒé˜¶æ®µæ£€ç´¢â€”â€”`é‡æ’` (Rerank)ï¼Œå³åˆ©ç”¨ `é‡æ’æ¨¡å‹`ï¼ˆRerankerï¼‰ï¼Œä½¿å¾—è¶Šç›¸ä¼¼çš„ç»“æœæ’åæ›´é å‰ã€‚è¿™æ ·å°±èƒ½å®ç°å‡†ç¡®ç‡ç¨³å®šå¢é•¿ï¼Œå³æ•°æ®è¶Šå¤šï¼Œæ•ˆæœè¶Šå¥½ï¼ˆå¦‚ä¸Šå›¾ä¸­ç´«çº¿æ‰€ç¤ºï¼‰ã€‚

é€šå¸¸ï¼Œä¸ºäº†ä¸ `é‡æ’` è¿›è¡ŒåŒºåˆ†ï¼Œä¸€é˜¶æ®µæ£€ç´¢æœ‰æ—¶ä¹Ÿè¢«ç§°ä¸º `ç²¾æ’` ã€‚è€Œåœ¨ä¸€äº›æ›´å¤æ‚çš„ç³»ç»Ÿä¸­ï¼Œåœ¨ `å¬å›` å’Œ `ç²¾æ’` ä¹‹é—´è¿˜ä¼šæ·»åŠ ä¸€ä¸ª `ç²—æ’` æ­¥éª¤ï¼Œè¿™é‡Œä¸å†å±•å¼€ï¼Œæ„Ÿå…´è¶£çš„åŒå­¦å¯ä»¥è‡ªè¡Œæœç´¢ã€‚

ç»¼ä¸Šæ‰€è¿°ï¼Œåœ¨æ•´ä¸ª `æ£€ç´¢` è¿‡ç¨‹ä¸­ï¼Œè®¡ç®—é‡çš„é¡ºåºæ˜¯ `å¬å›` > `ç²¾æ’` > `é‡æ’`ï¼Œè€Œæ£€ç´¢æ•ˆæœçš„é¡ºåºåˆ™æ˜¯ `å¬å›` < `ç²¾æ’` < `é‡æ’` ã€‚

å½“è¿™ä¸€å¤æ‚çš„ `æ£€ç´¢` è¿‡ç¨‹å®Œæˆåï¼Œæˆ‘ä»¬å°±ä¼šå¾—åˆ°æ’å¥½åºçš„ä¸€ç³»åˆ— `æ£€ç´¢æ–‡æ¡£`ï¼ˆRetrieval Documentsï¼‰ã€‚

ç„¶åæˆ‘ä»¬ä¼šä»å…¶ä¸­æŒ‘é€‰æœ€ç›¸ä¼¼çš„ `k` ä¸ªç»“æœï¼Œå°†å®ƒä»¬å’Œç”¨æˆ·æŸ¥è¯¢æ‹¼æ¥æˆpromptçš„å½¢å¼ï¼Œè¾“å…¥åˆ°å¤§æ¨¡å‹ã€‚

æœ€åï¼Œå¤§å‹æ¨¡å‹å°±èƒ½å¤Ÿä¾æ®æ‰€æä¾›çš„çŸ¥è¯†æ¥ç”Ÿæˆå›å¤ï¼Œä»è€Œæ›´æœ‰æ•ˆåœ°è§£ç­”ç”¨æˆ·çš„é—®é¢˜

è‡³æ­¤ï¼Œä¸€ä¸ªå®Œæ•´çš„RAGé“¾è·¯å°±æ„å»ºå®Œæ¯•äº†ã€‚

### 1.2 å¼€æºRAGæ¡†æ¶

ç›®å‰ï¼Œå¼€æºç¤¾åŒºä¸­å·²ç»æ¶Œç°å‡ºäº†ä¼—å¤šRAGæ¡†æ¶ï¼Œä¾‹å¦‚ï¼š

- [TinyRAG](https://github.com/KMnO4-zx/TinyRAG)ï¼šDataWhaleæˆå‘˜å®‹å¿—å­¦ç²¾å¿ƒæ‰“é€ çš„çº¯æ‰‹å·¥æ­å»ºRAGæ¡†æ¶ã€‚
- [LlamaIndex](https://github.com/run-llama/llama_index)ï¼šä¸€ä¸ªç”¨äºæ„å»ºå¤§è¯­è¨€æ¨¡å‹åº”ç”¨ç¨‹åºçš„æ•°æ®æ¡†æ¶ï¼ŒåŒ…æ‹¬æ•°æ®æ‘„å–ã€æ•°æ®ç´¢å¼•å’ŒæŸ¥è¯¢å¼•æ“ç­‰åŠŸèƒ½
- [LangChain](https://github.com/langchain-ai/langchain)ï¼šä¸€ä¸ªä¸“ä¸ºå¼€å‘å¤§è¯­è¨€æ¨¡å‹åº”ç”¨ç¨‹åºè€Œè®¾è®¡çš„æ¡†æ¶ï¼Œæä¾›äº†æ„å»ºæ‰€éœ€çš„æ¨¡å—å’Œå·¥å…·ã€‚
- [QAnything](https://github.com/netease-youdao/QAnything)ï¼šç½‘æ˜“æœ‰é“å¼€å‘çš„æœ¬åœ°çŸ¥è¯†åº“é—®ç­”ç³»ç»Ÿï¼Œæ”¯æŒä»»æ„æ ¼å¼æ–‡ä»¶æˆ–æ•°æ®åº“ã€‚
- [RAGFlow](https://github.com/infiniflow/ragflow)ï¼šInfiniFlowå¼€å‘çš„åŸºäºæ·±åº¦æ–‡æ¡£ç†è§£çš„RAGå¼•æ“ã€‚
- Â·Â·Â·

è¿™äº›å¼€æºé¡¹ç›®å„å…·ä¼˜åŠ¿ï¼ŒåŠŸèƒ½ä¸°å¯Œï¼Œæå¤§çš„æ¨åŠ¨äº†RAGæŠ€æœ¯çš„å‘å±•ã€‚
ç„¶è€Œï¼Œéšç€è¿™äº›æ¡†æ¶åŠŸèƒ½çš„ä¸æ–­æ‰©å±•ï¼Œå­¦ä¹ è€…ä¸å¯é¿å…åœ°éœ€è¦æ‰¿æ‹…è¾ƒé«˜çš„å­¦ä¹ æˆæœ¬ã€‚

å› æ­¤ï¼Œæœ¬èŠ‚è¯¾å°†ä»¥ `Yuan2-2B-Mars` æ¨¡å‹ä¸ºåŸºç¡€ï¼Œè¿›è¡ŒRAGå®æˆ˜ã€‚å¸Œæœ›é€šè¿‡æ„å»ºä¸€ä¸ªç®€åŒ–ç‰ˆçš„RAGç³»ç»Ÿï¼Œæ¥å¸®åŠ©å¤§å®¶æŒæ¡RAGçš„æ ¸å¿ƒæŠ€æœ¯ï¼Œä»è€Œè¿›ä¸€æ­¥äº†è§£ä¸€ä¸ªå®Œæ•´çš„RAGé“¾è·¯ã€‚

## å°è¯•å®ä¾‹

### ä¸‹è½½æ–‡ä»¶

```Bash
git lfs install
git clone https://www.modelscope.cn/datasets/Datawhale/AICamp_yuan_baseline.git
cp AICamp_yuan_baseline/Task\ 3ï¼šæºå¤§æ¨¡å‹RAGå®æˆ˜/* .
pip install streamlit == 1.24.0
```

### 2.2 æ¨¡å‹ä¸‹è½½

åœ¨RAGå®æˆ˜ä¸­ï¼Œæˆ‘ä»¬éœ€è¦æ„å»ºä¸€ä¸ªå‘é‡æ¨¡å‹ã€‚

å‘é‡æ¨¡å‹é€šå¸¸é‡‡ç”¨BERTæ¶æ„ï¼Œå®ƒæ˜¯ä¸€ä¸ªTransformer Encoderã€‚

è¾“å…¥å‘é‡æ¨¡å‹å‰ï¼Œé¦–å…ˆä¼šåœ¨æ–‡æœ¬çš„æœ€å‰é¢é¢å¤–åŠ ä¸€ä¸ª `[CLS]` tokenï¼Œç„¶åå°†è¯¥tokenæœ€åä¸€å±‚çš„éšè—å±‚å‘é‡ä½œä¸ºæ–‡æœ¬çš„è¡¨ç¤ºã€‚å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š
![image.png](https://cdn.jsdelivr.net/gh/BlackJack0083/image@main/img/20240825140714.png)

ç›®å‰ï¼Œå¼€æºçš„åŸºäºBERTæ¶æ„çš„å‘é‡æ¨¡å‹æœ‰å¦‚ä¸‹ï¼š
- [BGE Embedding](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/baai_general_embedding)ï¼šæ™ºæºé€šç”¨embeddingï¼ˆBAAI general embedding, BGEï¼‰
- [BCEmbedding](https://github.com/netease-youdao/BCEmbedding)ï¼šç½‘æ˜“æœ‰é“è®­ç»ƒçš„Bilingual and Crosslingual Embedding
- [jina-embeddings](https://huggingface.co/jinaai/jina-embeddings-v2-base-zh)ï¼šJina AIè®­ç»ƒçš„text embedding
- [M3E](https://huggingface.co/moka-ai/m3e-large)ï¼šMokaAIè®­ç»ƒçš„ Massive Mixed Embedding
- Â·Â·Â·

é™¤äº†BERTæ¶æ„ä¹‹å¤–ï¼Œè¿˜æœ‰åŸºäºLLMçš„å‘é‡æ¨¡å‹æœ‰å¦‚ä¸‹ï¼š
- [LLM-Embedder](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/llm_embedder)ï¼šæ™ºæºLLM-Embedder
- Â·Â·Â·

å…¶æ¬¡ï¼Œè¿˜æœ‰API:
- [OpenAI API](https://platform.openai.com/docs/guides/embeddings)
- [Jina AI API](https://jina.ai/embeddings/)
- [ZhipuAI API](https://open.bigmodel.cn/dev/api#text_embedding)
- Â·Â·Â·

åœ¨æœ¬æ¬¡å­¦ä¹ ä¸­ï¼Œæˆ‘ä»¬é€‰ç”¨åŸºäºBERTæ¶æ„çš„å‘é‡æ¨¡å‹ `bge-small-zh-v1.5`ï¼Œå®ƒæ˜¯ä¸€ä¸ª4å±‚çš„BERTæ¨¡å‹ï¼Œæœ€å¤§è¾“å…¥é•¿åº¦512ï¼Œè¾“å‡ºçš„å‘é‡ç»´åº¦ä¹Ÿä¸º512ã€‚

`bge-small-zh-v1.5` æ”¯æŒé€šè¿‡å¤šä¸ªå¹³å°è¿›è¡Œä¸‹è½½ï¼Œå› ä¸ºæˆ‘ä»¬çš„æœºå™¨å°±åœ¨é­”æ­ï¼Œæ‰€ä»¥è¿™é‡Œæˆ‘ä»¬ç›´æ¥é€‰æ‹©é€šè¿‡é­”æ­è¿›è¡Œä¸‹è½½ã€‚

æ¨¡å‹åœ¨é­”æ­å¹³å°çš„åœ°å€ä¸º [AI-ModelScope/bge-small-zh-v1.5](https://modelscope.cn/models/AI-ModelScope/bge-small-zh-v1.5)ã€‚

```Python
# å‘é‡æ¨¡å‹ä¸‹è½½
from modelscope import snapshot_download
model_dir = snapshot_download("AI-ModelScope/bge-small-zh-v1.5", cache_dir='.')

# æºå¤§æ¨¡å‹ä¸‹è½½
from modelscope import snapshot_download
model_dir = snapshot_download('IEITYuan/Yuan2-2B-Mars-hf', cache_dir='.')
```

### 2.3 RAGå®æˆ˜

æ¨¡å‹ä¸‹è½½å®Œæˆåï¼Œå°±å¯ä»¥å¼€å§‹RAGå®æˆ˜å•¦ï¼

#### 2.3.1 **ç´¢å¼•**

ä¸ºäº†æ„é€ ç´¢å¼•ï¼Œè¿™é‡Œæˆ‘ä»¬å°è£…äº†ä¸€ä¸ªå‘é‡æ¨¡å‹ç±» `EmbeddingModel`ï¼š

```Python
# å®šä¹‰å‘é‡æ¨¡å‹ç±»
class EmbeddingModel:
    """
    class for EmbeddingModel
    """

    def __init__(self, path: str) -> None:
        # åˆå§‹åŒ–å‡½æ•°ï¼Œæ¥æ”¶æ¨¡å‹çš„è·¯å¾„ä½œä¸ºå‚æ•°
        self.tokenizer = AutoTokenizer.from_pretrained(path)
        # åŠ è½½ä¸æ¨¡å‹è·¯å¾„å¯¹åº”çš„Tokenizer

        self.model = AutoModel.from_pretrained(path).cuda()
        # åŠ è½½ä¸æ¨¡å‹è·¯å¾„å¯¹åº”çš„æ¨¡å‹ï¼Œå¹¶å°†å…¶ç§»åŠ¨åˆ°GPUä¸Š
        print(f'Loading EmbeddingModel from {path}.')

    def get_embeddings(self, texts: List) -> List[float]:
        """
        è®¡ç®—æ–‡æœ¬åˆ—è¡¨çš„åµŒå…¥å‘é‡
        å‚æ•°:
            texts: è¦å¤„ç†çš„æ–‡æœ¬åˆ—è¡¨
        è¿”å›:
            sentence_embeddings: æ–‡æœ¬çš„åµŒå…¥å‘é‡åˆ—è¡¨
        """
        # ä½¿ç”¨tokenizerå¤„ç†æ–‡æœ¬ï¼Œè¿›è¡Œåˆ†è¯ã€å¡«å……(padding)å’Œæˆªæ–­(truncation)
        encoded_input = self.tokenizer(texts, padding=True, truncation=True, return_tensors='pt')
        # å°†ç¼–ç åçš„è¾“å…¥æ•°æ®ç§»åŠ¨åˆ°GPUä¸Š
        encoded_input = {k: v.cuda() for k, v in encoded_input.items()}
        
        with torch.no_grad():
            # ä¸è®¡ç®—æ¢¯åº¦ï¼Œä»¥åŠ å¿«è®¡ç®—é€Ÿåº¦
            model_output = self.model(**encoded_input)
            # ä»æ¨¡å‹è¾“å‡ºä¸­è·å–æœ€åä¸€å±‚çš„éšè—çŠ¶æ€
            sentence_embeddings = model_output[0][:, 0]
        # å°†å¥å­çš„åµŒå…¥å‘é‡æ ‡å‡†åŒ–ï¼Œä½¿å…¶å…·æœ‰å•ä½èŒƒæ•°
        sentence_embeddings = torch.nn.functional.normalize(sentence_embeddings, p=2, dim=1)
        
        # å°†åµŒå…¥å‘é‡è½¬æ¢ä¸ºåˆ—è¡¨æ ¼å¼å¹¶è¿”å›
        return sentence_embeddings.tolist()
```

é€šè¿‡ä¼ å…¥æ¨¡å‹è·¯å¾„ï¼Œæ–°å»ºä¸€ä¸ª `EmbeddingModel` å¯¹è±¡ `embed_model`ã€‚

åˆå§‹åŒ–æ—¶è‡ªåŠ¨åŠ è½½å‘é‡æ¨¡å‹çš„tokenizerå’Œæ¨¡å‹å‚æ•°ã€‚

> åœ¨Pythonä¸­ï¼Œ`->`ç¬¦å·é€šå¸¸ç”¨äºç±»å‹æ³¨è§£ï¼Œè¡¨ç¤ºå‡½æ•°è¿”å›å€¼çš„é¢„æœŸç±»å‹ã€‚å®ƒæ˜¯ç±»å‹æç¤ºï¼ˆType Hintsï¼‰çš„ä¸€éƒ¨åˆ†ï¼Œç”¨äºå‘Šè¯‰è§£é‡Šå™¨å‡½æ•°çš„å‚æ•°å’Œè¿”å›å€¼åº”è¯¥æ˜¯ä»€ä¹ˆç±»å‹ã€‚ç±»å‹æç¤ºå¯ä»¥å¸®åŠ©å¼€å‘è€…é¿å…ç±»å‹é”™è¯¯ï¼Œå¹¶ä¸”å¯ä»¥è¢«IDEå’Œé™æ€ç±»å‹æ£€æŸ¥å·¥å…·ä½¿ç”¨æ¥æé«˜ä»£ç çš„å¯è¯»æ€§å’Œå¥å£®æ€§ã€‚

```Python
print("> Create embedding model...")
embed_model_path = './AI-ModelScope/bge-small-zh-v1___5'
embed_model = EmbeddingModel(embed_model_path)
```

#### 2.3.2 æ£€ç´¢

ä¸ºäº†å®ç°å‘é‡æ£€ç´¢ï¼Œæˆ‘ä»¬å®šä¹‰äº†ä¸€ä¸ªå‘é‡åº“ç´¢å¼•ç±» `VectorStoreIndex`ï¼š

```Python
# å®šä¹‰å‘é‡åº“ç´¢å¼•ç±»
class VectorStoreIndex:
    """
    class for VectorStoreIndex
    """

    def __init__(self, document_path: str, embed_model: EmbeddingModel) -> None:
        # åˆå§‹åŒ–å‡½æ•°ï¼Œæ¥æ”¶æ–‡æ¡£è·¯å¾„å’ŒåµŒå…¥æ¨¡å‹ä½œä¸ºå‚æ•°
        self.documents = []
        for line in open(document_path, 'r', encoding='utf-8'):
            line = line.strip()
            self.documents.append(line)  # è¯»å–æ–‡æ¡£å¹¶å»é™¤ç©ºç™½å­—ç¬¦ï¼Œç„¶åæ·»åŠ åˆ°æ–‡æ¡£åˆ—è¡¨

        self.embed_model = embed_model  # å­˜å‚¨åµŒå…¥æ¨¡å‹å®ä¾‹
        self.vectors = self.embed_model.get_embeddings(self.documents)  # ä¸ºæ‰€æœ‰æ–‡æ¡£è·å–åµŒå…¥å‘é‡

        print(f'Loading {len(self.documents)} documents for {document_path}.')  # æ‰“å°åŠ è½½æ–‡æ¡£æ•°é‡å’Œè·¯å¾„

    def get_similarity(self, vector1: List[float], vector2: List[float]) -> float:
        """
        è®¡ç®—ä¸¤ä¸ªå‘é‡ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦
        """
        dot_product = np.dot(vector1, vector2)  # è®¡ç®—ä¸¤ä¸ªå‘é‡çš„ç‚¹ç§¯
        magnitude = np.linalg.norm(vector1) * np.linalg.norm(vector2)  # è®¡ç®—ä¸¤ä¸ªå‘é‡çš„æ¨¡é•¿ä¹˜ç§¯
        if not magnitude:  # å¦‚æœæ¨¡é•¿ä¹˜ç§¯ä¸º0ï¼Œé¿å…é™¤ä»¥0
            return 0
        return dot_product / magnitude  # è¿”å›ä½™å¼¦ç›¸ä¼¼åº¦å€¼

    def query(self, question: str, k: int = 1) -> List[str]:
        """
        æ ¹æ®é—®é¢˜æŸ¥è¯¢æœ€ç›¸ä¼¼çš„æ–‡æ¡£
        å‚æ•°:
            question: è¾“å…¥çš„é—®é¢˜æ–‡æœ¬
            k: è¿”å›æœ€ç›¸ä¼¼çš„æ–‡æ¡£æ•°é‡ï¼Œé»˜è®¤ä¸º1
        è¿”å›:
            æŸ¥è¯¢ç»“æœçš„æ–‡æ¡£åˆ—è¡¨
        """
        question_vector = self.embed_model.get_embeddings([question])[0]  # ä¸ºé—®é¢˜æ–‡æœ¬è·å–åµŒå…¥å‘é‡
        result = np.array([self.get_similarity(question_vector, vector) for vector in self.vectors])  # è®¡ç®—é—®é¢˜å‘é‡ä¸æ‰€æœ‰æ–‡æ¡£å‘é‡çš„ç›¸ä¼¼åº¦
        return np.array(self.documents)[result.argsort()[-k:][::-1]].tolist()  # è¿”å›æœ€ç›¸ä¼¼çš„kä¸ªæ–‡æ¡£

# æ³¨æ„ï¼šä»£ç ä¸­ä½¿ç”¨äº†numpyåº“æ¥è®¡ç®—å‘é‡æ“ä½œï¼Œéœ€è¦å¯¼å…¥numpyåº“
# ä¾‹å¦‚ï¼šimport numpy as np
# åŒæ—¶ï¼ŒListç±»å‹éœ€è¦ä»typingæ¨¡å—å¯¼å…¥
# ä¾‹å¦‚ï¼šfrom typing import List

print("> Create index...")
doecment_path = './knowledge.txt'
index = VectorStoreIndex(doecment_path, embed_model)
```

ä¸Šæ–‡æåˆ° `get_embeddings()` å‡½æ•°æ”¯æŒä¸€æ¬¡æ€§ä¼ å…¥å¤šæ¡æ–‡æœ¬ï¼Œä½†ç”±äºGPUçš„æ˜¾å­˜æœ‰é™ï¼Œè¾“å…¥çš„æ–‡æœ¬ä¸å®œå¤ªå¤šã€‚
æ‰€ä»¥ï¼Œå¦‚æœçŸ¥è¯†åº“å¾ˆå¤§ï¼Œéœ€è¦å°†çŸ¥è¯†åº“åˆ‡åˆ†æˆå¤šä¸ªbatchï¼Œç„¶ååˆ†æ‰¹æ¬¡é€å…¥å‘é‡æ¨¡å‹ã€‚
è¿™é‡Œï¼Œå› ä¸ºæˆ‘ä»¬çš„çŸ¥è¯†åº“æ¯”è¾ƒå°ï¼Œæ‰€ä»¥å°±ç›´æ¥ä¼ åˆ°äº† `get_embeddings()` å‡½æ•°ã€‚

å…¶æ¬¡ï¼Œ`VectorStoreIndex` ç±»è¿˜æœ‰ä¸€ä¸ª `get_similarity()` å‡½æ•°ï¼Œå®ƒç”¨äºè®¡ç®—ä¸¤ä¸ªå‘é‡ä¹‹é—´çš„ç›¸ä¼¼åº¦ï¼Œè¿™é‡Œé‡‡ç”¨äº†ä½™å¼¦ç›¸ä¼¼åº¦ã€‚

æœ€åï¼Œæˆ‘ä»¬ä»‹ç»ä¸€ä¸‹ `VectorStoreIndex` ç±»çš„å…¥å£ï¼Œå³æŸ¥è¯¢å‡½æ•° `query()`ã€‚ä¼ å…¥ç”¨æˆ·çš„æé—®åï¼Œé¦–å…ˆä¼šé€å…¥å‘é‡æ¨¡å‹è·å¾—å…¶å‘é‡è¡¨ç¤ºï¼Œç„¶åä¸çŸ¥è¯†åº“ä¸­çš„æ‰€æœ‰å‘é‡è®¡ç®—ç›¸ä¼¼åº¦ï¼Œæœ€åå°† `k` ä¸ªæœ€ç›¸ä¼¼çš„æ–‡æ¡£æŒ‰é¡ºåºè¿”å›ï¼Œ`k`é»˜è®¤ä¸º1ã€‚

è¿™é‡Œæˆ‘ä»¬ä¼ å…¥ç”¨æˆ·é—®é¢˜ `ä»‹ç»ä¸€ä¸‹å¹¿å·å¤§å­¦`ï¼Œå¯ä»¥çœ‹åˆ°ï¼Œå‡†ç¡®åœ°è¿”å›äº†çŸ¥è¯†åº“ä¸­çš„ç¬¬ä¸€æ¡çŸ¥è¯†ã€‚
![image.png](https://cdn.jsdelivr.net/gh/BlackJack0083/image@main/img/20240825144237.png)

#### 2.3.3 ç”Ÿæˆ

ä¸ºäº†å®ç°åŸºäºRAGçš„ç”Ÿæˆï¼Œæˆ‘ä»¬è¿˜éœ€è¦å®šä¹‰ä¸€ä¸ªå¤§è¯­è¨€æ¨¡å‹ç±» `LLM`ï¼š

```Python
# å®šä¹‰å¤§è¯­è¨€æ¨¡å‹ç±»
class LLM:
    """
    class for Yuan2.0 LLM
    """

    def __init__(self, model_path: str) -> None:
        print("Creating tokenizer...")
        # åˆå§‹åŒ–Tokenizerï¼Œä¸æ·»åŠ EOSå’ŒBOSæ ‡è®°ï¼Œè€Œæ˜¯ä½¿ç”¨è‡ªå®šä¹‰çš„EOSæ ‡è®°
        self.tokenizer = AutoTokenizer.from_pretrained(model_path, add_eos_token=False, add_bos_token=False, eos_token='<eod>')
        # æ·»åŠ ç‰¹æ®Šæ ‡è®°åˆ°Tokenizerä¸­ï¼Œè¿™äº›æ ‡è®°å°†ç”¨äºæ¨¡å‹çš„ä¸åŒéƒ¨åˆ†
        self.tokenizer.add_tokens(['<sep>', '<pad>', '<mask>', '<predict>', '<FIM_SUFFIX>', '<FIM_PREFIX>', '<FIM_MIDDLE>',
                                   '<commit_before>', '<commit_msg>', '<commit_after>', '<jupyter_start>',
                                   '<jupyter_text>', '<jupyter_code>', '<jupyter_output>', '<empty_output>'], special_tokens=True)

        print("Creating model...")
        # åˆå§‹åŒ–æ¨¡å‹ï¼Œä½¿ç”¨åŠç²¾åº¦æµ®ç‚¹æ•°æ¥å‡å°‘å†…å­˜ä½¿ç”¨ï¼Œå¹¶å°†æ¨¡å‹ç§»åŠ¨åˆ°GPUä¸Š
        self.model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.bfloat16, trust_remote_code=True).cuda()

        print(f'Loading Yuan2.0 model from {model_path}.')

    def generate(self, question: str, context: List):
        # æ ¹æ®é—®é¢˜å’Œä¸Šä¸‹æ–‡ç”Ÿæˆå›ç­”
        if context:
            # å¦‚æœæä¾›äº†ä¸Šä¸‹æ–‡ï¼Œåˆ™æ„å»ºä¸€ä¸ªæç¤ºï¼ŒåŒ…æ‹¬èƒŒæ™¯å’Œé—®é¢˜
            prompt = f'èƒŒæ™¯ï¼š{context}\né—®é¢˜ï¼š{question}\nè¯·åŸºäºèƒŒæ™¯ï¼Œå›ç­”é—®é¢˜ã€‚'
        else:
            # å¦‚æœæ²¡æœ‰æä¾›ä¸Šä¸‹æ–‡ï¼Œåªä½¿ç”¨é—®é¢˜ä½œä¸ºæç¤º
            prompt = question

        # å°†æç¤ºæ·»åŠ åˆ†éš”ç¬¦<sep>ï¼Œå¹¶ä½¿ç”¨Tokenizerè¿›è¡Œç¼–ç 
        prompt += "<sep>"
        inputs = self.tokenizer(prompt, return_tensors="pt")["input_ids"].cuda()
        
        # ä½¿ç”¨æ¨¡å‹ç”Ÿæˆå›ç­”ï¼Œä¸è¿›è¡Œé‡‡æ ·ï¼ˆdo_sample=Falseï¼‰ï¼Œæœ€å¤§é•¿åº¦é™åˆ¶ä¸º1024
        outputs = self.model.generate(inputs, do_sample=False, max_length=1024)
        
        # å°†ç”Ÿæˆçš„tokenè§£ç å›æ–‡æœ¬
        output = self.tokenizer.decode(outputs[0])
        
        # æ‰“å°è¾“å‡ºæ–‡æœ¬ï¼Œåªæ˜¾ç¤ºåˆ†éš”ç¬¦<sep>ä¹‹åçš„éƒ¨åˆ†
        print(output.split("<sep>")[-1])

# æ³¨æ„ï¼šä»£ç ä¸­ä½¿ç”¨äº†AutoTokenizer, AutoModelForCausalLMç­‰ç±»ï¼Œè¿™äº›ç±»é€šå¸¸æ¥è‡ªHugging Faceçš„Transformersåº“ã€‚
# æ­¤å¤–ï¼ŒListç±»å‹æ³¨è§£éœ€è¦ä»typingæ¨¡å—å¯¼å…¥ï¼Œtorch.bfloat16æ˜¯PyTorchä¸­çš„æ•°æ®ç±»å‹ï¼Œç”¨äºæŒ‡å®šæ¨¡å‹ä½¿ç”¨çš„ç²¾åº¦ã€‚


print("> Create Yuan2.0 LLM...")
model_path = './IEITYuan/Yuan2-2B-Mars-hf'
llm = LLM(model_path)

```

1. `__init__`ï¼šæ„é€ å‡½æ•°ï¼Œç”¨äºåˆå§‹åŒ–Tokenizerå’Œæ¨¡å‹ï¼Œå¹¶å°†æ¨¡å‹ç§»åŠ¨åˆ°GPUä¸Šä»¥åŠ é€Ÿè®¡ç®—ã€‚æ­¤å¤–ï¼Œè¿˜æ·»åŠ äº†ä¸€ç³»åˆ—ç‰¹æ®Šæ ‡è®°åˆ°Tokenizerä¸­ã€‚
2. `generate`ï¼šè¯¥æ–¹æ³•æ¥å—ä¸€ä¸ªé—®é¢˜å’Œå¯é€‰çš„ä¸Šä¸‹æ–‡ä½œä¸ºè¾“å…¥ï¼Œæ„å»ºä¸€ä¸ªæç¤ºï¼Œç„¶åä½¿ç”¨æ¨¡å‹ç”Ÿæˆå›ç­”ã€‚ç”Ÿæˆçš„å›ç­”å°†è¢«è§£ç å¹¶æ‰“å°å‡ºæ¥ï¼Œåªæ˜¾ç¤ºåˆ†éš”ç¬¦`<sep>`ä¹‹åçš„éƒ¨åˆ†ã€‚

```Python
print('> Without RAG:')
llm.generate(question, [])

print('> With RAG:')
# æ ¹æ®é—®é¢˜å†…å®¹æ‰¾åˆ°æœ€åŒ¹é…çš„æ–‡æœ¬contentsï¼Œå°†å…¶é™„åŠ åœ¨prompté‡Œé¢
llm.generate(question, context)
```
![image.png](https://cdn.jsdelivr.net/gh/BlackJack0083/image@main/img/20240825145556.png)

## 3 è¯¾åä½œä¸š

æ„å»ºä¸€ä¸ªçŸ¥è¯†åº“ï¼Œä½¿ç”¨`Yuan2-2B`æ¨¡å‹è¿›è¡ŒRAGå®æˆ˜ï¼Œå¯¹æ¯”ä½¿ç”¨å’Œä¸ä½¿ç”¨RAGçš„æ•ˆæœã€‚
### 3.1 çŸ¥è¯†åº“æ„å»º
- ç¡®å®šæƒ³è¦å¼€å‘åº”ç”¨æ‰€åœ¨çš„é¢†åŸŸï¼šå¤§æ¨¡å‹åœ¨å“ªäº›é¢†åŸŸè¿˜å­˜åœ¨é—®é¢˜ï¼Ÿèƒ½å¦é€šè¿‡çŸ¥è¯†åº“è¿›è¡Œè§£å†³ï¼Ÿ
- æ”¶é›†é¢†åŸŸæ•°æ®ï¼šè¯¥é¢†åŸŸçš„æ•°æ®æœ‰å“ªäº›æ¥æºï¼Ÿç™¾åº¦ç™¾ç§‘ï¼Ÿä¹¦ç±ï¼Ÿè®ºæ–‡ï¼Ÿ
- æ„å»ºçŸ¥è¯†åº“ï¼šæ”¶é›†å¥½çš„æ•°æ®éœ€è¦å“ªäº›é¢„å¤„ç†æ­¥éª¤ï¼Ÿæ•°æ®æ¸…æ´—ï¼Ÿå»é‡ï¼Ÿ
### 3.2 RAGå®æˆ˜
- çŸ¥è¯†åº“ç´¢å¼•ï¼šçŸ¥è¯†åº“æ€ä¹ˆåˆ‡åˆ†æ•ˆæœæœ€å¥½ï¼Ÿè€ƒè™‘åˆ°æ•ˆæœå’Œæ•ˆç‡ç­‰å› ç´ ï¼Œå“ªä¸ªå‘é‡æ¨¡å‹æ›´é€‚é…ï¼Ÿ
- æ£€ç´¢ï¼šå¦‚æœçŸ¥è¯†åº“æ¯”è¾ƒå¤§æˆ–è€…ä¸ºäº†å®ç°æ›´å¿«çš„æ£€ç´¢ï¼Œéœ€è¦å“ªäº›å·¥å…·ï¼Ÿ
- ç”Ÿæˆï¼šæ£€ç´¢å‡ºå†…å®¹æ€ä¹ˆèƒ½è¢«å¤§æ¨¡å‹åˆ©ç”¨å¥½ï¼Ÿpromptæ€ä¹ˆè°ƒä¼˜ï¼Ÿ

### 3.3 å…¶ä»–
å‚è€ƒä¸‹é¢èµ„æ–™ï¼Œæ¢ç´¢RAGæ¡†æ¶çš„ä½¿ç”¨æ–¹æ³•ï¼š
- langchainï¼š[https://github.com/datawhalechina/llm-cookbook](https://github.com/datawhalechina/llm-cookbook/tree/main/content/%E9%80%89%E4%BF%AE-Building%20and%20Evaluating%20Advanced%20RAG%20Applications)
- llamaIndexï¼š[https://github.com/datawhalechina/llm-universe](https://github.com/datawhalechina/llm-universe/tree/main/notebook/C4%20%E6%9E%84%E5%BB%BA%20RAG%20%E5%BA%94%E7%94%A8)

# AI ç§‘ç ”åŠ©æ‰‹

